import text

def test_normalize_casefold_and_spaces():
    s = "–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t"
    assert text.normalize(s) == "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"


def test_normalize_yo2e():
    s = "—ë–∂–∏–∫, –Å–ª–∫–∞"
    assert text.normalize(s) == "–µ–∂–∏–∫, –µ–ª–∫–∞"


def test_normalize_crlf():
    s = "Hello\r\nWorld"
    assert text.normalize(s) == "hello world"


def test_normalize_double_spaces():
    s = "  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "
    assert text.normalize(s) == "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"


def test_tokenize_simple_ru():
    assert text.tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä") == ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]


def test_tokenize_punct():
    assert text.tokenize("hello,world!!!") == ["hello", "world"]


def test_tokenize_hyphen():
    assert text.tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ") == ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]


def test_tokenize_numbers():
    assert text.tokenize("2025 –≥–æ–¥") == ["2025", "–≥–æ–¥"]


def test_tokenize_emoji_filtered():
    assert text.tokenize("emoji üòÉ –Ω–µ —Å–ª–æ–≤–æ") == ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]


def test_count_freq_and_top_n_basic():
    tokens = ["a", "b", "a", "c", "b", "a"]
    freq = text.count_freq(tokens)
    assert freq == {"a": 3, "b": 2, "c": 1}
    assert text.top_n(freq, n=2) == [("a", 3), ("b", 2)]


def test_top_n_tie_alpha_sort():
    tokens = ["bb", "aa", "bb", "aa", "cc"]
    freq = text.count_freq(tokens)
    assert freq == {"aa": 2, "bb": 2, "cc": 1}
    assert text.top_n(freq, n=2) == [("aa", 2), ("bb", 2)]